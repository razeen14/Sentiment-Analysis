# -*- coding: utf-8 -*-
"""Sentiment Analysis on IMDB Reviews with LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-DmMWAAxaKEbH5Xyf3M1ilzjR3ZiSiku
"""

!pip install kaggle

"""**Importing the Dependencies**"""

import os
import json

from zipfile import ZipFile
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""**Data Collection**"""

kaggle_dictionary = json.load(open("kaggle.json"))

# setup kaggle credentials as environment variables

os.environ["KAGGLE_USERNAME"] = kaggle_dictionary["username"]
os.environ["KAGGLE_KEY"] = kaggle_dictionary["key"]

!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews

# unzipping

with ZipFile("/content/imdb-dataset-of-50k-movie-reviews.zip", "r") as zip_ref  :
  zip_ref.extractall()

"""**Loading The Dataset**"""

data = pd.read_csv("/content/IMDB Dataset.csv")

data.shape

data.head()

data.tail()

data["sentiment"].value_counts()

data.replace({"sentiment" : {"positive" : 1, "negative" : 0}}, inplace=True)

data["sentiment"].value_counts()

# split data into train data & test data

train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

print(train_data.shape)
print(test_data.shape)

"""**Data Preprocessing**"""

# Tokenize text data

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(train_data["review"])
X_train = pad_sequences(tokenizer.texts_to_sequences(train_data["review"]), maxlen=200)
X_test = pad_sequences(tokenizer.texts_to_sequences(test_data["review"]), maxlen=200)

print(X_train)

print(X_test)

Y_train = train_data["sentiment"]
Y_test = test_data["sentiment"]

print(Y_train)

print(Y_test)

"""**LSTM - Long Short Term Memory**"""

# build the model

model = Sequential()
model.add(Embedding(input_dim=5000,output_dim=128, input_length=200))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation="sigmoid"))

model.summary()

# compile the model

model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

"""**Training the Model**"""

model.fit(X_train, Y_train, epochs=10, batch_size=64, validation_split=0.2)

"""**Model Evaluation**"""

loss, accuracy = model.evaluate(X_test, Y_test)
print(f"Test Loss : {loss}")
print(f"Test Accuracy : {accuracy}")

"""**Building a Predictive System**"""

def predictive_sentiment(review):
  sequence = tokenizer.texts_to_sequences([review])
  padded_sequence = pad_sequences(sequence, maxlen=200)
  prediction = model.predict(padded_sequence)
  sentiment = "positive" if prediction[0][0] > 0.5 else "negative"
  return sentiment

new_review1 = "The movie was good, I love it"
sentiment1 = predictive_sentiment(new_review1)
print(f"The sentiment of the review is : {sentiment1}")

new_review2 = "The movie was not so good, I didn't love it"
sentiment2 = predictive_sentiment(new_review2)
print(f"The sentiment of the review is : {sentiment2}")